---
slug: "kuzu-0.4.0-release"
title: "KÃ¹zu 0.4.0 Release"
description: "Release announcement for KÃ¹zu 0.4.0"
pubDate: "April 29 2024"
heroImage: "/img/default.png"
categories: ["release"]
authors: ["team"]
tags: ["cypher", "extensions", "polars", "pyarrow"]
---

With the warmer weather in ðŸ‡¨ðŸ‡¦ approaching, we are excited on more than one front - we've just released
version **0.4.0** of KÃ¹zu! This is a significant release, as it introduces a new storage layer along with
a host of additional features, improvements and extensions, detailed in this post. Let's gear up!

## Features

### Import/Export database

You can now migrate databases between different KÃ¹zu versions without manually writing DDL and `COPY` statements.
In the background, KÃ¹zu generates the required DDL statements to facilitate the transfer of data out
of one version and into another. The `EXPORT DATABASE` command allows you to export the contents of
the database to a specific directory. The query below exports the database to an absolute path,
`/path/to/export`, utilizing the same configuration parameters as `COPY FROM` statements.

```cypher
EXPORT DATABASE TO '/path/to/export' (HEADER=true);
```

Exporting a database generates three Cypher files, one each for the schema, macro definitions and
the `COPY` statements for each table. In addition, the data files are exported to CSV (by default),
or Parquet, if desired.

The `IMPORT DATABASE` command allows you to import the contents of
a database from a specific directory. The query below imports the database from `/path/to/export`.

```cypher
IMPORT DATABASE FROM '/path/to/export';
```

The aim of this functionality is to make it easier to migrate databases between different
KÃ¹zu versions as we continue to evolve the core and the storage layer in future versions, and to
facilitate the sharing of databases between users over longer time periods.

### `COPY FROM` with subquery

In v0.4.0, we added support for using subqueries following the `COPY FROM` statement. This feature allows you to
first perform a task like `MATCH` and then use the results of that query as input to the `COPY FROM` command.

For example, consider that we have a graph with a `User` node label and a `Follows` relationship type.
We want to create a new `Person` node table and a `Knows` relationship table, where the goal is to state that
a `Person` "knows" another `Person` if they follow each other. We can use the `COPY FROM` command with a subquery
to achieve this as follows:

```cypher
// Create node/rel tables
CREATE NODE TABLE Person(name STRING, PRIMARY KEY (name));
CREATE REL TABLE Knows(FROM Person TO Person);
// Run COPY FROM with a subquery
COPY Person FROM (MATCH (a:User) RETURN a.name);
COPY Knows FROM (MATCH (a:User)-[r:Follows]->(b:User) RETURN a.name, b.name);
```

Using subqueries with `COPY FROM` opens up a wider range of possibilities for data manipulation and
transformation prior to inserting data into the database.

### Bulk insert into a non-empty table

In prior releases, the `COPY FROM` command could only be used to bulk insert data into an empty table.
This restriction has now been removed, and in v0.4.0, you can also bulk insert data into a non-empty table,
making it easier to append data to an existing table. This approach is likely much more efficient than
inserting data a record at a time via the `MERGE` command, especially when you have large amounts of data to append
to an existing table.

Below show an example of how `COPY FROM` might be used in conjunction with the subquery feature described
earlier. We have a single table named `Person` for two CSV files that have the same structure.

```cypher
// Create node table
CREATE NODE TABLE Person(name STRING, age INT64, PRIMARY KEY (name));
// Run COPY FROM with a subquery
COPY Person FROM (LOAD FROM "person1.csv" RETURN *);
COPY Person FROM (LOAD FROM "person2.csv" RETURN *);
```

Note that the usual primary key constraints still apply; i.e., if the file `person2.csv` contains a record
whose primary key already exists in the `Person` table, it will produce a `RuntimeError` and the
transaction will be rolled back.

### Scan from Pandas PyArrow backend

Earlier versions of KÃ¹zu provided the ability to scan data from a Pandas DataFrame using the NumPy backend.
We are very happy to announce that in v0.4.0, we added support for PyArrow-backed Pandas DataFrames as well.
Make sure to run `pip install -U pyarrow pandas` before trying the example below.

```py
import kuzu
import pandas as pd

db = kuzu.Database("persons")
conn = kuzu.Connection(db)

# Convert the Pandas DataFrame to a PyArrow-backed DataFrame
df = pd.DataFrame({
    "name": ["Adam", "Karissa", "Zhang", "Noura"],
    "age": [30, 40, 50, 25]
}).convert_dtypes(dtype_backend="pyarrow")

# Scan the PyArrow-backed Pandas DataFrame in KÃ¹zu by referencing the DataFrame object
result = conn.execute("LOAD FROM df RETURN *;")
print(result.get_as_df())
```

```bash
      name  age
0     Adam   30
1  Karissa   40
2    Zhang   50
3    Noura   25
```

How does this work under the hood? Internally, KÃ¹zu uses the same layout as an Arrow `Array`, allowing
it to perform a [memcpy](https://cplusplus.com/reference/cstring/memcpy/) operation,
which is more efficient than a conventional copy.
Using `memcpy` means we directly accesse the values in the memory blocks of the underlying Arrow objects, avoiding the
need to move data from the DataFrame's location in memory.

As Pandas 2.0 evolves, it is adding more and more support for Arrow-backed DataFrames in Python. Using
the PyArrow backend in Pandas offers [numerous benefits](https://pandas.pydata.org/docs/user_guide/pyarrow.html)
over the NumPy backend, including better support for strings and nulls, improved performance and better
interoperability with other full-fledged Arrow-backed DataFrame libraries (like Polars, cuDF, etc.).

### Better integration with Polars

Although this feature came out in a minor release just prior to this one (v0.3.2), it's worth mentioning here.
KÃ¹zu now allows directly outputting the results of a Cypher query as a Polars DataFrame. The query
results are converted to an Arrow table obtained via our `get_as_arrow()`
method, and then seamlessly passed to a Polars DataFrame via the Polars `from_arrow()` method. This feature
was made possible thanks to an [external contributor](https://github.com/kuzudb/kuzu/pull/2985)
via relatively few lines of code, shown below.

```py
import polars as pl

# class QueryResult:
    # ...
    # ...
    def get_as_pl(self) -> pl.DataFrame:
        return pl.from_arrow(data=self.get_as_arrow())
```

With [Apache Arrow](https://arrow.apache.org/) becoming the de facto standard for columnar data interchange in the Python ecosystem, we think
that future versions of KÃ¹zu could benefit from native scanning of Polars DataFrames in Python, in a
similar way to how we now scan PyArrow-backed Pandas DataFrames.

### New extensions: DuckDB and PostgreSQL

In v0.2.0, we introduced the idea of the KÃ¹zu extensions framework and our first extension, `httpfs`. In
v0.4.0, we are excited to introduce two brand new external database extensions: DuckDB and PostgreSQL.
For now, these are read-only extensions, allowing you to natively scan data from either database (no write support).

Using these extensions is remarkably simple: You first install the extension and then load it into your
KÃ¹zu session. The following snippet shows how this is done via the CLI:

```sql
INSTALL <extension_name>;
LOAD EXTENSION <extension_name>;
```

For the external database extensions, `<extension_name>` would be either `duckdb` or `postgres`. The way
this works in KÃ¹zu is demonstrated below in the case of PostgreSQL. You first
attach a Postgres database and its associated tables (while providing the necessary connection string parameters)
to your KÃ¹zu CLI or client session.
Under the hood, KÃ¹zu's cache stores the associated database schema, so your Cypher queries
are aware of the existing tables and their columns.

```sql
ATTACH 'dbname=university user=postgres host=localhost password=yourpassword port=5432' AS db1 (dbtype 'postgres');
```

Once the database is attached, you can run Cypher queries to scan the underlying tables, allowing you
to then much more easily construct a graph from the relational data.

```sql
LOAD FROM db1.person
RETURN *
```

We also make it possible to list which databases (and their types) are attached to a given KÃ¹zu session.

```cypher
CALL show_attached_databases() RETURN *;
// Output
------------------------------------
| name             | database type |
------------------------------------
| my_duckdb        | DUCKDB        |
------------------------------------
| my_postgres_db   | POSTGRES      |
------------------------------------
```

The main motivation of external database extensions is to make it easier to build graphs to analyze data
from multiple external sources, which are typically relational systems in large organizations.
More such extensions are planned in the future to make the process of analyzing graph-like data
obtained from various sources even more seamless.

## Performance improvements

This section highlights some of the performance improvements in v0.4.0.

### Bulk insertion into non-empty database

Below, we show the performance improvements in bulk insertion into a non-empty database. We compare the
initial bulk insertion performance with the subsequent bulk insertion performance.

#### Initial bulk insertion

-Insert table here-

#### Subsequent bulk insertion

-Insert table here-

### Internal ID compression

We now apply compression to the internal IDs of nodes and relationships in the storage layer. This
results in significant reduction in the size of a KÃ¹zu database. For
LDBC SF100, [we observed](https://github.com/kuzudb/kuzu/pull/3116) a **45%** reduction in size for
the `data.kz` file within the KÃ¹zu database directory.

| Version | Size of `data.kz` for LDBC SF100 |
| :---: | :---: |
| 0.3.0 | 126 GB |
| **0.4.0** | **69 GB** |

## Closing Remarks

This post highlighted only a few of the many features and improvements that came along with the v0.4.0 release.
It's recommended to check out our [release notes]() on GitHub for a more comprehensive list.

We are excited to bring these enhancements to the ever-growing KÃ¹zu user community. As always,
our many thanks go out to everyone in the KÃ¹zu team, including our interns and our external contributors
for their excellent work in making this release possible. We encourage you to try out the latest
release on your own workflows and engage with us on [Discord](https://discord.gg/VtX2gw9Rug)!