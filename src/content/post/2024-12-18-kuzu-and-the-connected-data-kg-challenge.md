---
slug: "kuzu-and-the-connected-data-kg-challenge"
title: "KÃ¹zu and the Connected Data London Knowledge Graph challenge"
description: "How we used KÃ¹zu to build a Graph RAG pipeline for the Connected Data London KG challenge"
pubDate: "Dec 18 2024"
heroImage: "/img/kuzu-cdkg/cdkg.png"
categories: ["example"]
authors: ["prashanth"]
tags: ["rag", "knowledge graph", "graph", "graphrag"]
draft: false
---

It's been an eventful end to the year for KÃ¹zu Inc.! Some members of our team were in London ðŸ‡¬ðŸ‡§ in early December to participate in [Connected Data London 2024](https://www.connected-data.london/),
a leading graph-focused event that brings together professionals in the fields of semantic technologies,
knowledge graphs, databases, data science, and AI. The event was packed with interesting talks, workshops
and demos, and they're well worth a watch on their YouTube channel once the recordings are available.

From a KÃ¹zu perspective, one of the highlights of the event was the
[Connected Data Knowledge Graph (CDKG) challenge](https://2024.connected-data.london/talks/the-connected-data-knowledge-graph-a-knowledge-graph-for-the-community-by-the-community/),
which I had the pleasure of participating in, right from its inception. The CDKG challenge originated from
a round table discussion in July 2024, in which a simple proof-of-concept was demonstrated -- the demo showed
the results of a simple Graph RAG pipeline that utilized a graph constructed from the unstructured text of
talk transcripts from past Connected Data World events. However, as is the case with most PoCs, numerous issues with the pipeline were identified,
such as missing links between entities, missing entity types, and duplicate entities. This led to the creation of the CDKG challenge,
described as "*A Knowledge Graph for the community by the community*"[^1], and it was launched as an open source
project [on GitHub](https://github.com/Connected-Data/cdkg-challenge) in the summer of 2024.

In this blog post, I'll walk through how we approached the problem of graph construction for the challenge
using KÃ¹zu as the underlying graph store, and how we built a Graph RAG pipeline on top of it. Along the way,
I hope it'll become clear how useful KÃ¹zu can be for facilitating the rapid iteration and experimentation
that is required for building more robust Graph RAG pipelines.

## Problem statement

As stated in [their vision](https://github.com/Connected-Data/cdkg-challenge?tab=readme-ov-file#our-vision),
the broader goals of the CDKG challenge are to build a curated knowledge graph of the collective wisdom of the material
from 260+ experts who have contributed to the event since its inception in 2016. The curated graph would
make it easier to discover, explore, digest, combine and reuse the collective knowledge in the community.
The underlying graph could be made available to the larger community for querying and exploration.

At KÃ¹zu, we are motivated by the goal of helping developers more easily use graphs in their applications,
so we embarked upon the CDKG challenge by dividing the problem into two parts:

- Graph construction: Construct a KÃ¹zu graph using a combination of hand-curated domain knowledge,
as well as the unstructured text of past events' talk transcripts.
- Graph RAG: Build a Graph RAG pipeline on top of the graph and subjectively evaluate the results.

Let's go over each of these parts in more detail below.

## Part I: Graph construction

The key steps in the graph construction phase are expressed via the following diagram:

<Image src="/img/kuzu-cdkg/cdl-graph-construction.png" />

The steps are performed in the following order:

1. Obtain data sources
2. Information extraction via scraping, transcription, LLM or ML-based extraction, etc.
3. Metamodel definition (more on this [below](#metamodel))
4. Semantic model definition for graph construction

### Data sources

The Connected Data London team provided participants with a curated [metadata file](https://github.com/Connected-Data/cdkg-challenge/tree/main/Transcripts)
in `.csv` format that contains metadata about past events, speakers, talks, and other entities based on the website of the past events.
In addition, we were given the text transcripts of the talks as subtitle (`.srt`) files that were generated by transcription tools.

The combination of these two data sources: The metadata CSV file, and the `.srt` files, are what are used
as the data sources for the graph construction phase.

### Information extraction

The unstructured text in the `.srt` files is first preprocessed to remove
the noise, such as timestamps, and the cleaned text is stored in `.txt` format. These text files contain the raw,
unfiltered text of what each speaker spoke in their talk, and provide useful knowledge about what topic keywords were discussed.
Because multiple speakers can each mention the
same keywords, such as "RDF" or "GraphQL" in their talks, we thought it would be useful to have our graph connect
the talks to the keywords they mention, so that we can traverse the graph to find
all the speakers who mentioned a particular keyword.

In our case, we used an OpenAI LLM (`gpt-4o-mini`) to extract keywords of technologies and frameworks
from the text files using an appropriate prompt[^2] that output the results for each talk in JSON format.
We could, of course, do more prompt engineering to extract more information from the talk transcripts,
but for this initial implementation, we only extracted keywords of technologies and frameworks that were
mentioned in the talks.

The example below shows the JSON output of the extracted keywords, or `Tag` entities in our property graph,
for the talk titled "Graph Thinking" by Paco Nathan.

```json
[
  {
    "filename": "Graph Thinking _ Paco Nathan _ Connected Data World 2021.txt",
    "entities": {
      "tag": [
        "rdf",
        "spark",
        "dask",
        "ray",
        "rapids",
        "apache arrow",
        "apache parquet",
        "pyvis",
        "matplotlib",
        "graphistry",
        "networkx",
        "igraph",
        "cugraph",
        "pytorch geometric"
      ]
    }
  }
]
```

In addition to using LLMs to extract keywords from the talk transcripts, we also considered other ways to scrape
additional metadata from the event websites, for example, the speaker affiliations, organization descriptions,
etc. However, in the interest of time, this part was left out for the initial implementation.

### Metamodel

Because the CDKG challenge is designed to be agnostic to both the graph database and the semantic model used downstream, we first
define what we call a *metamodel* that allows practitioners to first understand the structure of the
data before settling on their *semantic model* -- i.e. whether they will model the data using RDF
or property graphs.

The metamodel is defined as the set of concepts, entities, relationships and attributes that form the **domain knowledge**.
You can think of the metamodel as a high-level data model that is abstracted away from the contents of the data itself,
because it models how real-world entities (speakers, talks, events, etc.) are related to each other. You can
choose to model the attributes via RDF triples, or as properties on nodes and relationships in a property graph.

<Image src="/img/kuzu-cdkg/cdl-metamodel.png" />

You can see a markdown version of the metamodel [here](https://github.com/Connected-Data/cdkg-challenge/blob/main/Data%20Model/Connected_Data_Knowledge_Graph_Metamodel.md).

The metamodel definition stage is a very important one, because it allows practitioners to use their domain knowledge
to capture the inherent structure in the data prior to actually building the graph via the semantic model.
In our case, we choose to use KÃ¹zu as the graph database downstream, but the metamodel's
goal is to just as easily generalize the domain's data model to other graph technologies, whether it
be an RDF or a property graph.

### Semantic model

The semantic modelling phase is broken down into two components: the *domain graph* and the *content graph* (also known as the *lexical graph*)[^5].
The following subsections will highlight these different components.


#### Domain graph

The domain graph in KÃ¹zu is constructed using the information provided in the metadata CSV file.
KÃ¹zu is an embeddable property graph database that supports Cypher. Once the metamodel is defined as shown
above, it's relatively straightforward to transform the metamodel into a property graph schema in KÃ¹zu
that we can then use to construct the graph. Visually, the property graph schema in KÃ¹zu looks like the following:

<Image src="/img/kuzu-cdkg/cdl-schema.png" />

The `Speaker`, `Talk`, `Category` and `Event` entities are modelled as nodes, and the relevant metadata fields for each
entity as obtained from the metadata CSV file are stored as properties on the nodes. The directions of
the relationships are chosen based on our best judgment for easy querying. For example,
`(:Speaker)-[:GIVES_TALK]->(:Talk)` is an intuitive way to express that a speaker gives a talk,
both for humans and for LLMs that write Cypher.

The domain graph is constructed and stored in KÃ¹zu and can be visualized in [KÃ¹zu explorer](https://docs.kuzudb.com/visualization/).
The large green nodes represent each talk's `Category` ("Graph AI", "Semantic Technology", etc.), and the smaller blue nodes are the `Speaker` nodes, which are
connected to the red `Talk` nodes. The purple nodes represent the `Event` that a talk belongs to. For this
graph, we have two events: "Connected Data World 2021" and "Knowledge Connexions 2020".

<Image src="/img/kuzu-cdkg/domain-graph.png" />

The only missing piece in the domain graph compared to what was shown above in the 
metamodel is the `Organization` that each `Speaker` is affiliated with. These nodes were not
modelled because it wasn't straightforward to obtain this information from the Connected Data World
website. This can be addressed via better scraping methods and added to the graph in future iterations of the challenge.

#### Content (lexical) graph

We extend the domain graph by adding the `Tag` entities of keywords from the talk transcripts that were extracted
using an LLM, as shown above. The `Tag` entities are connected to the `Talk` entities via the `IS_DESCRIBED_BY` relationship.
The content graph is sometimes referred to as a *lexical graph*, because it models relationships between
lexical items (e.g. keywords, entities, etc.) in the text.

<Image src="/img/kuzu-cdkg/cdl-schema-content.png" />

Combining the domain graph and the content graph this way gives us a richer view of the data that allows us
to answer queries about talk content, such as "*Which speakers gave talks about 'RDF'?*". The full graph
is visualized below, where the orange nodes represent the `Tag` keywords that are described
by a given `Talk`.

<Image src="/img/kuzu-cdkg/full-graph.png" />

### Summary of the workflow

The graph shown above is constructed using a series of Python scripts provided in the [GitHub repo](https://github.com/Connected-Data/cdkg-challenge/tree/main/src/kuzu).
KÃ¹zu's strong level of integration with the Python AI and data ecosystem is apparent in this workflow, where we use Polars,
a popular DataFrame library in Python, to transform and manipulate the metadata CSV file as per our desired data model.
We leverage KÃ¹zu's [`COPY FROM`](https://docs.kuzudb.com/import/copy-from-dataframe/#polars)
command to bulk-ingest the contents of the Polars DataFrame into the database in a single line of code.


<Image src="/img/kuzu-cdkg/cdl-part-1-workflow.png" />

## Part II: Graph RAG

Let's now use the KÃ¹zu graph to answer some questions about our data! The Graph RAG pipeline consists of a two-stage process:

- **Text2Cypher**: Use an LLM to generate a Cypher query that is then run on the database. To generate syntactically correct Cypher,
we pass the schema of the graph as context to the LLM's prompt[^3].
- **Generation**: Use the retrieved results from the database as context to another LLM prompt[^4] to generate a response to the question.

The following image demonstrates this using the example question "*Which speakers gave talks about 'RDF'?*". Due to
the prompts used, the `gpt-4o-mini` LLM is able to generate syntactically correct Cypher queries that respect
the schema of the graph quite reliably across a range of questions.

<Image src="/img/kuzu-cdkg/cdl-part-2-workflow.png" />

The above methodology works quite well for a range of questions. Some examples, along with the generated Cypher queries, are listed below.

---

__*Q1: Which speakers gave a talk whose title contains the term 'Knowledge Mesh'? Please give the talk's full title and the names of the speakers.*__

```cypher
MATCH (s:Speaker)-[:GIVES_TALK]->(t:Talk) 
WHERE CONTAINS(LOWER(t.title), 'knowledge mesh') 
RETURN t.title, s.name
```
```
The talk titled "Knowledge Mesh: From Data Silos to Data Fabric at Global 2000 Enterprises" was given by speakers David Amzallag and Szymon Klarman.
```

---


__*Q2: What was discussed in the talk by Paco Nathan?*__

```cypher
MATCH (s:Speaker)-[:GIVES_TALK]->(t:Talk) 
WHERE LOWER(s.name) = 'paco nathan' 
RETURN t.title, t.description
```
```
In the talk by Paco Nathan, titled "Graph Thinking," he explored the concept of Graph Thinking as a method for conceptualizing problems that can be addressed using graph technologies. The discussion included parallels to learning theory, illustrating how individuals organize knowledge into graph-like structures as they advance from novice to expert in a field.

Nathan introduced intuitive examples that demonstrate the capabilities and trade-offs of graph technologies, aimed at domain experts to help initiate new graph projects. He shared insights gained from business use cases in industrial AI within the EU, focusing on overcoming cognitive challenges organizations encounter during large graph initiatives.

The talk also reviewed common industry use cases and discussed how graph data science practices can be implemented using Python open source tools. Additionally, Nathan presented the 'kglab' abstraction layer, which integrates various libraries into the PyData stack for enhanced graph technology utilization.
```

---

__*Q3: Can you tell me more about the event Connected Data World 2021?*__

```cypher
MATCH (e:Event) WHERE LOWER(e.name) = 'connected data world 2021' RETURN e
```
```
Connected Data World 2021, previously known as Connected Data London, is a premier event focused on Knowledge Graphs, Graph Data Science, AI, Graph Databases, and Semantic Technology. The event aims to bring together leaders and innovators in these fields, providing a platform to share insights and a visionary outlook on Graph as a foundational technology stack for the 2020s.
```

---

In Q1 and Q2, the Cypher queries generated by the LLM correctly match on lowercase strings, increasing the
chances of a match in the graph, and the Cypher query returned the correct properties that were then
correctly used by the generation LLM to answer the question. Q3 asks about the a specific event, "Connected Data World 2021", which
is a property value of an `Event` node in the graph. As can be seen, the LLM correctly writes a Cypher query that matches on *only* that
node and returns the event's `description` property for the generation LLM's context.
Our prompt engineering worked reasonably well for these questions!

That being said, the above Graph RAG pipeline has its limitations. If the user question is phrased
using terminology that's not present in the graph, or specified in partial form (e.g., "Connected Data World" instead of "Connected Data World 2021"),
the LLM may not be able to resolve the question correctly and it may write an incorrect Cypher query,
resulting an empty retrieval response. In such cases, some fallback mechanism is needed to
either rewrite the query in a different form, or to use an additional retrieval (for e.g., vector or keyword-based search)
to answer the question. There are also a whole host of agentic approaches that can be used to further enhance this pipeline.

## Key takeaways

If you've worked on traditional RAG applications before, you've likely applied vector search as the default retrieval
method. I hope that this blog post has shown that it's possible to use a curated
knowledge graph for the retrieval step in cases like this where the underlying data has inherent structure.
With a sufficiently high quality graph, it's possible to answer a surprisingly wide range of questions
using *just* the graph, an appropriate LLM within a Text2Cypher pipeline, and some basic prompt engineering! Creating
a metamodel upfront that captures the high-level structure of the data is also a useful exercise, as it
facilitates a deeper understanding of the data and the domain, prior to modelling the graph and storing it in a graph database.

It's definitely worth extending the Graph RAG pipeline shown to include vector embeddings of the unstructured
text of the talk transcripts, as this will improve the quality of the retrieval in cases where the graph
traversal does not yield a response. On that note, KÃ¹zu is on the verge of
releasing a native disk-based HNSW vector index that allows users to run similarity search queries in Cypher,
so this will be a useful future addition to the Graph RAG pipeline. Stay tuned for more updates on this in a future blog post!

In summary, there are numerous other ways to improve the Graph RAG pipeline. Some examples include:

- Extract other kinds of entities (e.g. people, places, organizations) and relationships from the transcripts using
alternative LLMs
- Extract more metadata from the data sources and add them to the domain graph to help answer more complex questions
- Explore vector search strategies in combination with the graph traversal
- Add better fallback mechanisms for cases where the graph traversal does not yield a response

The CDKG challenge was a fun and interesting project to cap off the year, and I'm really glad to have
participated in it. Many thanks to George Anadiotis from the Connected Data London team for initiating
this challenge, and to my collaborators Fidan Limani and Dennis Irorere for their contributions in
the metamodelling and experimentation phases of this project.

If you're interested in experimenting with KÃ¹zu and Graph RAG on your own projects, feel
free to browse through the data and code in the [GitHub repo](https://github.com/Connected-Data/cdkg-challenge),
or better yet, please contribute to the CDKG challenge by extending the graph with more metadata,
or by adding more sophisticated retrieval methods. And do reach out to us if you have any interesting
observations or implementations to share!

---

[^1]: The term "knowledge graph" throughout this post is used in a general sense (as was used in the
[description](https://2024.connected-data.london/talks/the-connected-data-knowledge-graph-a-knowledge-graph-for-the-community-by-the-community/)
of the CDKG challenge) and does not refer to a specific data model. Any reference to "knowledge graph" in relation to KÃ¹zu refers to the
underlying property graph in KÃ¹zu.

[^2]: You can see the custom prompt that used to extract keywords from the talk transcripts and output the results to JSON format
[here](https://github.com/Connected-Data/cdkg-challenge/blob/main/src/kuzu/01_extract_tag_keywords.py). The
[`ell`](http://docs.ell.so/) language model prompting framework was used for this task. A temperature of 0.0 was used for
the LLM to reduce the chance of the LLM hallucinating keywords that were not actually mentioned in the talk transcript.
The system and user prompts clearly specify the task and the format of the output, and the `gpt-4o-mini` LLM is able to extract the keywords
reasonably well.

[^3]: The system and user prompts used for the `gpt-4o-mini` LLM that generates the Cypher queries can be found
in [this file](https://github.com/Connected-Data/cdkg-challenge/blob/main/src/kuzu/rag.py). Once again,
[`ell`](http://docs.ell.so/) is the LLM prompting framework used. The temperature is once again set to 0.0
to reduce the chance of the LLM hallucinating property names, node labels, and relationship labels. Some KÃ¹zu-specific syntax
and functions are specified in the user prompt to help the LLM generate syntactically correct Cypher -- for example,
the `LOWER` function is required to match the case-insensitive nature of property names, and the `CONTAINS` function is used to match on
substrings in the the specified part of the query.

[^4]: The system and user prompts specified via [`ell`](http://docs.ell.so/) for the `gpt-4o-mini` LLM that generates the response to the question can be found in
[this file](https://github.com/Connected-Data/cdkg-challenge/blob/main/src/kuzu/rag.py). For text generation, we set the temperature to
be 0.3 to encourage the LLM to generate a more conversational response. As in most RAG pipelines, the system prompt in this case very explicitly
instructs the LLM to only use the provided context to answer the question, and not to make up the answer.

[^5]: [graphrag.com](https://graphrag.com/concepts/intro-to-graphrag/) is an open knowledge sharing initiative that aims to
catalog the various Graph RAG patterns and methodologies. The vocabulary on "domain graph" and "lexical graph" is inspired by the terminology
used in this website, so we encourage you to read more about the different patterns and techniques that the larger Graph RAG community
is exploring via this site.
